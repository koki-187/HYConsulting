#!/usr/bin/env python3
"""
å…¨å›½ä¸å‹•ç”£å–å¼•ãƒ‡ãƒ¼ã‚¿å–å¾—ãƒ»é›†è¨ˆã‚¹ã‚¯ãƒªãƒ—ãƒˆ

å›½åœŸäº¤é€šçœ ä¸å‹•ç”£æƒ…å ±ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‹ã‚‰å…¨å›½47éƒ½é“åºœçœŒã®ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã—ã€
é›†è¨ˆã—ã¦ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«æŠ•å…¥ã™ã‚‹ã€‚

ä½¿ç”¨æ–¹æ³•:
    python3 download-mlit-data.py --prefectures "æ±äº¬éƒ½,ç¥å¥ˆå·çœŒ,å¤§é˜ªåºœ" --years 5
"""

import argparse
import csv
import json
import os
import sys
import time
from collections import defaultdict
from datetime import datetime
from typing import Dict, List, Tuple

import mysql.connector
from dotenv import load_dotenv

# ç’°å¢ƒå¤‰æ•°èª­ã¿è¾¼ã¿
load_dotenv()

# éƒ½é“åºœçœŒãƒªã‚¹ãƒˆï¼ˆå…¨47éƒ½é“åºœçœŒï¼‰
ALL_PREFECTURES = [
    "åŒ—æµ·é“", "é’æ£®çœŒ", "å²©æ‰‹çœŒ", "å®®åŸçœŒ", "ç§‹ç”°çœŒ", "å±±å½¢çœŒ", "ç¦å³¶çœŒ",
    "èŒ¨åŸçœŒ", "æ ƒæœ¨çœŒ", "ç¾¤é¦¬çœŒ", "åŸ¼ç‰çœŒ", "åƒè‘‰çœŒ", "æ±äº¬éƒ½", "ç¥å¥ˆå·çœŒ",
    "æ–°æ½ŸçœŒ", "å¯Œå±±çœŒ", "çŸ³å·çœŒ", "ç¦äº•çœŒ", "å±±æ¢¨çœŒ", "é•·é‡çœŒ", "å²é˜œçœŒ",
    "é™å²¡çœŒ", "æ„›çŸ¥çœŒ", "ä¸‰é‡çœŒ", "æ»‹è³€çœŒ", "äº¬éƒ½åºœ", "å¤§é˜ªåºœ", "å…µåº«çœŒ",
    "å¥ˆè‰¯çœŒ", "å’Œæ­Œå±±çœŒ", "é³¥å–çœŒ", "å³¶æ ¹çœŒ", "å²¡å±±çœŒ", "åºƒå³¶çœŒ", "å±±å£çœŒ",
    "å¾³å³¶çœŒ", "é¦™å·çœŒ", "æ„›åª›çœŒ", "é«˜çŸ¥çœŒ", "ç¦å²¡çœŒ", "ä½è³€çœŒ", "é•·å´çœŒ",
    "ç†Šæœ¬çœŒ", "å¤§åˆ†çœŒ", "å®®å´çœŒ", "é¹¿å…å³¶çœŒ", "æ²–ç¸„çœŒ"
]

# ç‰©ä»¶ç¨®åˆ¥ãƒãƒƒãƒ”ãƒ³ã‚°
PROPERTY_TYPE_MAP = {
    "å®…åœ°(åœŸåœ°)": "åœŸåœ°",
    "å®…åœ°(åœŸåœ°ã¨å»ºç‰©)": "ä¸€æˆ¸å»ºã¦",
    "ä¸­å¤ãƒãƒ³ã‚·ãƒ§ãƒ³ç­‰": "ãƒãƒ³ã‚·ãƒ§ãƒ³",
    "è¾²åœ°": "è¾²åœ°",
    "æ—åœ°": "æ—åœ°"
}

# ç¯‰å¹´æ•°ã‚°ãƒ«ãƒ¼ãƒ—ãƒãƒƒãƒ”ãƒ³ã‚°
def get_building_age_group(year_str: str) -> str:
    """ç¯‰å¹´æ•°ã‚’å¹´æ•°ã‚°ãƒ«ãƒ¼ãƒ—ã«å¤‰æ›"""
    if not year_str or year_str == "":
        return "ä¸æ˜"
    
    try:
        # è¥¿æš¦ã‚’å–å¾—
        year = int(year_str)
        current_year = datetime.now().year
        age = current_year - year
        
        if age < 0:
            return "ä¸æ˜"
        elif age <= 5:
            return "0ï½5å¹´"
        elif age <= 10:
            return "5ï½10å¹´"
        elif age <= 15:
            return "10ï½15å¹´"
        elif age <= 20:
            return "15ï½20å¹´"
        elif age <= 25:
            return "20ï½25å¹´"
        elif age <= 30:
            return "25ï½30å¹´"
        else:
            return "30å¹´ä»¥ä¸Š"
    except:
        return "ä¸æ˜"


def aggregate_data(csv_file_path: str) -> List[Dict]:
    """
    CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿ã€é›†è¨ˆãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆ
    
    Returns:
        List of aggregated records
    """
    print(f"ğŸ“Š é›†è¨ˆå‡¦ç†é–‹å§‹: {csv_file_path}")
    
    # é›†è¨ˆç”¨è¾æ›¸
    aggregated = defaultdict(lambda: {
        "totalPriceYen": 0,
        "totalAreaM2": 0,
        "transactionCount": 0,
        "prices": [],
        "areas": []
    })
    
    try:
        with open(csv_file_path, 'r', encoding='utf-8-sig') as f:
            reader = csv.DictReader(f)
            
            for row in reader:
                # å¿…é ˆãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã®ç¢ºèª
                if not all(k in row for k in ['ç¨®é¡', 'éƒ½é“åºœçœŒå', 'å¸‚åŒºç”ºæ‘å', 'åœ°åŒºå', 'å–å¼•ä¾¡æ ¼ï¼ˆç·é¡ï¼‰', 'é¢ç©ï¼ˆã¡ï¼‰']):
                    continue
                
                # ãƒ‡ãƒ¼ã‚¿æŠ½å‡º
                property_type_raw = row.get('ç¨®é¡', '')
                property_type = PROPERTY_TYPE_MAP.get(property_type_raw, property_type_raw)
                prefecture = row.get('éƒ½é“åºœçœŒå', '')
                city = row.get('å¸‚åŒºç”ºæ‘å', '')
                district = row.get('åœ°åŒºå', '') or city  # åœ°åŒºåãŒç©ºã®å ´åˆã¯å¸‚åŒºç”ºæ‘åã‚’ä½¿ç”¨
                building_year = row.get('å»ºç¯‰å¹´', '')
                building_age_group = get_building_age_group(building_year)
                
                # ä¾¡æ ¼ã¨é¢ç©
                try:
                    price = float(row.get('å–å¼•ä¾¡æ ¼ï¼ˆç·é¡ï¼‰', '0').replace(',', ''))
                    area = float(row.get('é¢ç©ï¼ˆã¡ï¼‰', '0').replace(',', ''))
                except:
                    continue
                
                # ç„¡åŠ¹ãªãƒ‡ãƒ¼ã‚¿ã‚’ã‚¹ã‚­ãƒƒãƒ—
                if price <= 0 or area <= 0:
                    continue
                
                # é›†è¨ˆã‚­ãƒ¼
                key = (prefecture, city, district, property_type, building_age_group)
                
                # é›†è¨ˆ
                aggregated[key]["totalPriceYen"] += price
                aggregated[key]["totalAreaM2"] += area
                aggregated[key]["transactionCount"] += 1
                aggregated[key]["prices"].append(price)
                aggregated[key]["areas"].append(area)
        
        # é›†è¨ˆçµæœã‚’æ•´å½¢
        results = []
        for key, data in aggregated.items():
            prefecture, city, district, property_type, building_age_group = key
            
            # åªå˜ä¾¡è¨ˆç®—ï¼ˆ1åª = 3.30579ã¡ï¼‰
            avg_price_per_m2 = data["totalPriceYen"] / data["totalAreaM2"]
            price_per_tsubo = int(avg_price_per_m2 * 3.30579)
            
            # å¹³å‡å€¤è¨ˆç®—
            avg_price = int(sum(data["prices"]) / len(data["prices"]))
            avg_area = sum(data["areas"]) / len(data["areas"])
            
            results.append({
                "propertyType": property_type,
                "prefecture": prefecture,
                "city": city,
                "district": district,
                "buildingAgeGroup": building_age_group,
                "totalPriceYen": data["totalPriceYen"],
                "totalAreaM2": data["totalAreaM2"],
                "transactionCount": data["transactionCount"],
                "pricePerTsubo": price_per_tsubo,
                "averagePriceYen": avg_price,
                "averageAreaM2": avg_area
            })
        
        print(f"âœ… é›†è¨ˆå®Œäº†: {len(results)}ä»¶ã®ãƒ¬ã‚³ãƒ¼ãƒ‰")
        return results
    
    except Exception as e:
        print(f"âŒ ã‚¨ãƒ©ãƒ¼: {e}")
        return []


def insert_to_database(records: List[Dict], dataset_version_id: str):
    """
    é›†è¨ˆãƒ‡ãƒ¼ã‚¿ã‚’ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«æŠ•å…¥
    """
    print(f"ğŸ’¾ ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æŠ•å…¥é–‹å§‹: {len(records)}ä»¶")
    
    try:
        # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶š
        conn = mysql.connector.connect(
            **mysql.connector.connect.parse_dsn(os.getenv('DATABASE_URL'))
        )
        cursor = conn.cursor()
        
        # ãƒãƒƒãƒæŠ•å…¥
        batch_size = 100
        inserted_count = 0
        
        for i in range(0, len(records), batch_size):
            batch = records[i:i+batch_size]
            
            # INSERTæ–‡
            sql = """
                INSERT INTO aggregated_real_estate_data 
                (propertyType, prefecture, city, district, buildingAgeGroup,
                 totalPriceYen, totalAreaM2, transactionCount, pricePerTsubo,
                 averagePriceYen, averageAreaM2, datasetVersionId, createdAt)
                VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, NOW())
                ON DUPLICATE KEY UPDATE
                totalPriceYen = totalPriceYen + VALUES(totalPriceYen),
                totalAreaM2 = totalAreaM2 + VALUES(totalAreaM2),
                transactionCount = transactionCount + VALUES(transactionCount),
                pricePerTsubo = (totalPriceYen + VALUES(totalPriceYen)) / (totalAreaM2 + VALUES(totalAreaM2)) * 3.30579,
                averagePriceYen = (totalPriceYen + VALUES(totalPriceYen)) / (transactionCount + VALUES(transactionCount)),
                averageAreaM2 = (totalAreaM2 + VALUES(totalAreaM2)) / (transactionCount + VALUES(transactionCount))
            """
            
            values = [
                (
                    r["propertyType"], r["prefecture"], r["city"], r["district"],
                    r["buildingAgeGroup"], r["totalPriceYen"], r["totalAreaM2"],
                    r["transactionCount"], r["pricePerTsubo"], r["averagePriceYen"],
                    r["averageAreaM2"], dataset_version_id
                )
                for r in batch
            ]
            
            cursor.executemany(sql, values)
            conn.commit()
            
            inserted_count += len(batch)
            print(f"  é€²æ—: {inserted_count}/{len(records)} ({inserted_count*100//len(records)}%)")
        
        cursor.close()
        conn.close()
        
        print(f"âœ… ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æŠ•å…¥å®Œäº†: {inserted_count}ä»¶")
        
    except Exception as e:
        print(f"âŒ ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚¨ãƒ©ãƒ¼: {e}")
        raise


def main():
    parser = argparse.ArgumentParser(description='å›½åœŸäº¤é€šçœãƒ‡ãƒ¼ã‚¿å–å¾—ãƒ»é›†è¨ˆã‚¹ã‚¯ãƒªãƒ—ãƒˆ')
    parser.add_argument('--prefectures', type=str, help='å¯¾è±¡éƒ½é“åºœçœŒï¼ˆã‚«ãƒ³ãƒåŒºåˆ‡ã‚Šï¼‰', default='all')
    parser.add_argument('--years', type=int, help='å–å¾—å¹´æ•°ï¼ˆç›´è¿‘Nå¹´ï¼‰', default=5)
    parser.add_argument('--csv-dir', type=str, help='CSVãƒ•ã‚¡ã‚¤ãƒ«ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª', default='./mlit-data')
    parser.add_argument('--dry-run', action='store_true', help='ãƒ‰ãƒ©ã‚¤ãƒ©ãƒ³ï¼ˆãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æŠ•å…¥ãªã—ï¼‰')
    
    args = parser.parse_args()
    
    # å¯¾è±¡éƒ½é“åºœçœŒ
    if args.prefectures == 'all':
        target_prefectures = ALL_PREFECTURES
    else:
        target_prefectures = [p.strip() for p in args.prefectures.split(',')]
    
    print("=" * 60)
    print("ğŸš€ å…¨å›½ä¸å‹•ç”£ãƒ‡ãƒ¼ã‚¿å–å¾—ãƒ»é›†è¨ˆã‚¹ã‚¯ãƒªãƒ—ãƒˆ")
    print("=" * 60)
    print(f"å¯¾è±¡éƒ½é“åºœçœŒ: {len(target_prefectures)}ä»¶")
    print(f"å–å¾—å¹´æ•°: ç›´è¿‘{args.years}å¹´")
    print(f"CSVãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {args.csv_dir}")
    print(f"ãƒ‰ãƒ©ã‚¤ãƒ©ãƒ³: {args.dry_run}")
    print("=" * 60)
    
    # CSVãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªç¢ºèª
    if not os.path.exists(args.csv_dir):
        print(f"âŒ CSVãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒå­˜åœ¨ã—ã¾ã›ã‚“: {args.csv_dir}")
        print("\nğŸ“ æ‰‹å‹•ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰æ‰‹é †:")
        print("1. https://www.reinfolib.mlit.go.jp/realEstatePrices/ ã«ã‚¢ã‚¯ã‚»ã‚¹")
        print("2. éƒ½é“åºœçœŒãƒ»ç‰©ä»¶ç¨®åˆ¥ãƒ»æœŸé–“ã‚’é¸æŠ")
        print("3. ã€Œãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã€ãƒœã‚¿ãƒ³ã‚’ã‚¯ãƒªãƒƒã‚¯")
        print(f"4. ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ãŸCSVã‚’ {args.csv_dir}/ ã«é…ç½®")
        print(f"5. ãƒ•ã‚¡ã‚¤ãƒ«å: <éƒ½é“åºœçœŒ>_<ç‰©ä»¶ç¨®åˆ¥>_<æœŸé–“>.csv")
        return
    
    # CSVãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§å–å¾—
    csv_files = [f for f in os.listdir(args.csv_dir) if f.endswith('.csv')]
    
    if not csv_files:
        print(f"âŒ CSVãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {args.csv_dir}")
        return
    
    print(f"\nğŸ“‚ CSVãƒ•ã‚¡ã‚¤ãƒ«: {len(csv_files)}ä»¶")
    
    # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãƒãƒ¼ã‚¸ãƒ§ãƒ³ID
    dataset_version_id = f"mlit_aggregated_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
    
    # å„CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡¦ç†
    total_records = 0
    for csv_file in csv_files:
        csv_path = os.path.join(args.csv_dir, csv_file)
        print(f"\n{'='*60}")
        print(f"ğŸ“„ å‡¦ç†ä¸­: {csv_file}")
        print(f"{'='*60}")
        
        # é›†è¨ˆ
        records = aggregate_data(csv_path)
        total_records += len(records)
        
        # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æŠ•å…¥
        if not args.dry_run and records:
            insert_to_database(records, dataset_version_id)
        
        # ãƒ¬ãƒ¼ãƒˆåˆ¶é™å¯¾ç­–
        time.sleep(1)
    
    print(f"\n{'='*60}")
    print(f"ğŸ‰ å…¨å‡¦ç†å®Œäº†")
    print(f"{'='*60}")
    print(f"ç·ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°: {total_records}ä»¶")
    print(f"ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆID: {dataset_version_id}")
    print(f"{'='*60}")


if __name__ == "__main__":
    main()
